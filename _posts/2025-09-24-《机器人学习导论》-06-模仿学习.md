---
layout:     post
title:      《机器人学导论》笔记-06-模仿学习
subtitle:   Imitation Learning
date:       2025-09-24
author:     BXCX
header-img: img/rli-06/20250920-rob-bg-01.png
catalog: 	 true
tags:
    - 机器人学习
    - 具身智能
    - 模仿学习
---
## 大纲
- 符号说明，MDP，POMDP
- 为什么是模仿学习？
- 模仿学习为什么/为何不同于标准的监督学习？
  - 行为克隆（BC）有效吗？
- 如何解决这个问题？
- 通过生成模型进行深度模仿学习

***
### MDP
- 定义：
  - $S$是状态空间。$s_t\in S$是时间步$t$的状态。
  - $A$是动作空间。$a_t\in A$是时间步$t$的动作。
  - $O$是观测空间。$o_t\in O$是时间步$t$的观测。
  - $p$是一步转移概率（即动力学）：$s_{t+1}\sim p(\cdot|s_t,a_t)$
  - $h$是观测模型：$o_t\sim h(\cdot|s_t)$
  - $r: S\times A\to \mathbb{R}$是奖励函数

<br>
- 一个马尔可夫决策过程(MDP)是一个由($S,A,p,r$)组成的元组
  - 目标：学习一个策略$\pi_\theta(a_t|s_t)$
  <center>
  <img src="../img/rli-06/20250924-rli-06-pic-02.png" width="60%">
  </center>

***
### POMDP
- 定义：
  - $S$是状态空间。$s_t\in S$是时间步$t$的状态。
  - $A$是动作空间。$a_t\in A$是时间步$t$的动作。
  - $O$是观测空间。$o_t\in O$是时间步$t$的观测。
  - $p$是一步转移概率（即动力学）：$s_{t+1}\sim p(\cdot|s_t,a_t)$
  - $h$是观测模型：$o_t\sim h(\cdot|s_t)$
  - $r: S\times A\to \mathbb{R}$是奖励函数
<br>

- 部分观测马尔可夫决策过程(POMDP)是一个由($S,A,O,p,h,r$)组成的元组
  - 目标：学习一个策略$\pi_\theta(a_t|observations)$
  <center>
  <img src="../img/rli-06/20250924-rli-06-pic-03.png" width="60%">
  </center>

***
### 马尔可夫性质
- 定义：
  - $S$是状态空间。$s_t\in S$是时间步$t$的状态。
  - $A$是动作空间。$a_t\in A$是时间步$t$的动作。
  - $O$是观测空间。$o_t\in O$是时间步$t$的观测。
  - $p$是一步转移概率（即动力学）：$s_{t+1}\sim p(\cdot|s_t,a_t)$
  - $h$是观测模型：$o_t\sim h(\cdot|s_t)$
  - $r: S\times A\to \mathbb{R}$是奖励函数

<div style="display: flex;">
  <div style="width: 75%;">
<br>

- 为什么是马尔可夫？
  - 一旦状态已知，就可以舍弃历史信息。
  <img src="../img/rli-06/20250924-rli-06-pic-04.png" width="90%">
  </div>
  <div style="width: 25%;">
    <img src="../img/rli-06/20250924-rli-06-pic-05.png" width="80%">
  </div>
</div>

***
### 强化学习与控制的目标
- 有限时域的情况：$T$是有限的
- 无限时域的情况：$T=\infin$
- 对累积奖励进行折扣：$\sum_t\gamma^tr(s_t,a_t)$，这里$0<\gamma\leq1$
<center>
<img src="../img/rli-06/20250924-rli-06-pic-06.png" width="80%">
</center>

***
### 模仿学习 (Imitaion Learning)
- 核心思想
  - 收集专家数据（观测/状态和动作对）
  - 训练一个函数，将观察结果/状态映射到动作
<center>
<img src="../img/rli-06/20250924-rli-06-pic-07.png" width="80%">
</center>

***
### 今天的深度模仿学习
- 左边：移动ALOHA；右边：扩散策略
<img src="../img/rli-06/20250924-rli-06-pic-08.png" width="90%">

***
### 听起来像是监督学习
<div style="display: flex;">
  <div style="width: 40%;">
<br>

- 核心思想
  - 收集专家数据（观测/状态和动作对）
  - 训练一个函数，将观察结果/状态映射到动作
  - 又名，“行为克隆”(BC)
  </div>
  <div style="width: 60%;">
    <img src="../img/rli-06/20250924-rli-06-pic-09.png" width="80%">
  </div>
</div>

- 高维观测数据
<center>
<img src="../img/rli-06/20250924-rli-06-pic-10.png" width="70%">
</center>


***
### 它不是标准的监督学习
- 行为克隆(BC)有效吗？
- 行为克隆（BC）确实有效
- 但是小车很容易掉下去，那么问题出在哪里？
<center>
<img src="../img/rli-06/20250924-rli-06-pic-11.png" width="80%">
</center>

- 独立同分布假设不成立！
- 回顾一下监督学习
<center>
<img src="../img/rli-06/20250924-rli-06-pic-12.png" width="70%">
</center>

- $(x_i,y_i)$是从某个分布中独立同分布地采样的。
- 基本上，“低训练误差” -> “良好的测试性能”
- 专家数据只包含了“好”的行为
- 因此，测试时的一个小的错误，就会导致级联故障
<center>
<img src="../img/rli-06/20250924-rli-06-pic-13.png" width="80%">
</center>

- 回顾机器学习/深度学习中的域迁移
<center>
<img src="../img/rli-06/20250924-rli-06-pic-14.png" width="80%">
</center>

- 在连续决策中甚至更糟！域迁移时“动态的”和“致命的”
  - 策略仅使用专家数据进行训练
  - 专家几乎不可能看到“坏”状态
  - 从统计学上讲，模仿学习就像是**悬崖行走**
  - 一旦我们犯了一个错误，我们就脱离了训练分布

***
### 一些分析
- 考虑“悬崖行走”问题
  - 测试误差与时间步数的平方成正比
<center>
<img src="../img/rli-06/20250924-rli-06-pic-15.png" width="80%">
</center>

- 为什么这么悲观？
  - 事实上，我们常常可以从错误中**恢复**过来
  - 但这个**悖论**仍然存在：如果数据有更多的错误，IL可以工作的更好

***
### 怎么样解决这个问题？
- 明智的收集（并增强）我们的数据
- 改变算法（DAgger，Dataset Aggregation，数据集聚合）
- 下一步：使用非常强大的模型，使错误很少
- 使用多任务学习

<center>
<img src="../img/rli-06/20250924-rli-06-pic-16.png" width="80%">
</center>

***
### 什么使得BC容易/困难？
- 明智的收集（并增强）我们的数据
- 想法1：有意识的添加错误和修正
  - 错误有害，但修正有用，且比错误更有用
- 想法2：使用数据增强
  - 添加一些“伪造”或“合成”数据，以说明更正

<center>
<img src="../img/rli-06/20250924-rli-06-pic-17.png" width="70%">
</center>

***
### 数据增强案例
- 使用导管增强一个MPC(Model Predictive Control，模型预测控制)专家
<center>
<img src="../img/rli-06/20250924-rli-06-pic-18.png" width="60%">
</center>

- 干扰驾驶员转向信号 
<center>
<img src="../img/rli-06/20250924-rli-06-pic-19.png" width="70%">
</center>

***
### 核心思想：交互
<center>
<img src="../img/rli-06/20250924-rli-06-pic-20.png" width="80%">
</center>

#### 第一轮迭代
<center>
<img src="../img/rli-06/20250924-rli-06-pic-21.png" width="80%">
</center>

#### 第二轮迭代
<center>
<img src="../img/rli-06/20250924-rli-06-pic-22.png" width="80%">
</center>

***
### DAgger：数据集聚合（Dataset Aggregation）
#### 第二轮迭代
<center>
<img src="../img/rli-06/20250924-rli-06-pic-23.png" width="80%">
</center>

#### 第三轮迭代
<center>
<img src="../img/rli-06/20250924-rli-06-pic-24.png" width="80%">
</center>

#### 第n轮迭代
<center>
<img src="../img/rli-06/20250924-rli-06-pic-25.png" width="80%">
</center>

### 很有用！

### DAgger总结
- 它本质上是一个在线学习算法
  - 遗憾保证(With regret guarantees!)
- 问题：需要持续查询专家
<center>
<img src="../img/rli-06/20250924-rli-06-pic-26.png" width="70%">
</center>